{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import imageio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "import time\n",
    "import convis\n",
    "# This is module used to produce spikes. It is important to have the latest version 0.6.4 installed with:\n",
    "# pip install git+https://github.com/jahuth/convis.git\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' #Only needed for high resolution displays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['axes.edgecolor'] = 'white' \n",
    "mpl.rcParams['axes.labelcolor'] = 'white' \n",
    "mpl.rcParams['xtick.color'] = 'white' \n",
    "mpl.rcParams['ytick.color'] = 'white' \n",
    "mpl.rcParams['text.color'] = 'white' \n",
    "mpl.rcParams['axes.facecolor'] = '#111111'\n",
    "mpl.rcParams['figure.max_open_warning'] = 0\n",
    "#mpl.rcParams['lines.linewidth'] = .2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will group cell types according to the size of their receptive field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_cells = [1,14,16,19,21,23,24,26]\n",
    "large_cells = [6,7,8,9,11,12,13,22,28,34]\n",
    "medium_cells = [2,3,4,5,10,15,17,18,20,25,27,29,30,31,32,33,35,36,37,38,39]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(data, batch_size):\n",
    "    transformed = np.zeros((data.shape[0], batch_size, 1))\n",
    "    for i in range(data.shape[0]):\n",
    "        transformed[i, :, :] = data[i]\n",
    "\n",
    "    transformed = torch.from_numpy(transformed)\n",
    "    transformed = transformed.float()\n",
    "    return transformed\n",
    "\n",
    "def save(var, name):\n",
    "    file = open(name + \".pkl\", 'wb')\n",
    "    pickle.dump(var, file)\n",
    "    file.close()\n",
    "\n",
    "def load(file):\n",
    "    file = open(\"./\" + file + \".pkl\", 'rb')\n",
    "    var = pickle.load(file)\n",
    "    file.close()\n",
    "    return var\n",
    "\n",
    "def timeit(fun):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        t1 = time.time()\n",
    "        params = fun(*args, **kwargs)\n",
    "        t2 = time.time()\n",
    "        print(\"Time it took to run the function: {}\".format(t2 - t1))\n",
    "        return params\n",
    "    return wrapper\n",
    "\n",
    "def savemodel(model, name, root='./'):\n",
    "    torch.save(model.state_dict(), root + name)\n",
    "\n",
    "def loadmodel(model, name, root='./', cuda=True, gpu=0):\n",
    "    #device = torch.device(\"cuda:{}\".format(gpu) if cuda else \"cpu\")\n",
    "    if cuda:\n",
    "        model.load_state_dict(torch.load(root + file, map_location='cuda:{}'.format(gpu)))\n",
    "        model.to(device)\n",
    "        model.cuda(device)\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(root + name, map_location='cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "In order to import the pytorch trained models you first need to replicate the original class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lstmcell(nn.Module):\n",
    "    def __init__(self, device, hidden_size=51, batch_size=4):\n",
    "        super(Lstmcell, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.device = device\n",
    "        self.photo = nn.LSTMCell(1, self.hidden_size)\n",
    "        self.bipol = nn.LSTMCell(self.hidden_size, 1)\n",
    "\n",
    "    def init_weights(self):\n",
    "        h_photo = torch.zeros(self.batch_size, self.hidden_size)\n",
    "        c_photo = torch.zeros(self.batch_size, self.hidden_size)\n",
    "        h_bipol = torch.zeros(self.batch_size, 1)\n",
    "        c_bipol = torch.zeros(self.batch_size, 1)\n",
    "\n",
    "        h_photo, c_photo, h_bipol, c_bipol  =  (h_photo.to(self.device), c_photo.to(self.device),\n",
    "                                                h_bipol.to(self.device), c_bipol.to(self.device))\n",
    "        return h_photo, c_photo, h_bipol, c_bipol\n",
    "\n",
    "    def forward(self, stimulus):\n",
    "        h_photo, c_photo, h_bipol, c_bipol = self.init_weights()\n",
    "        output = torch.empty(stimulus.size())\n",
    "        for i in range(stimulus.shape[0]):\n",
    "            h_photo, c_photo = self.photo(stimulus[i], (h_photo, c_photo))\n",
    "            h_bipol, c_bipol = self.bipol(h_photo, (h_bipol, c_bipol))\n",
    "            output[i] = h_bipol\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class spkLstmcell(nn.Module):\n",
    "    def __init__(self, device, hidden_size=51, batch_size=4):\n",
    "        super(Lstmcell, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.device = device\n",
    "        self.photo = nn.LSTMCell(1, self.hidden_size)\n",
    "        self.bipol = nn.LSTMCell(self.hidden_size, 1)\n",
    "\n",
    "    def init_weights(self):\n",
    "        h_photo = torch.zeros(self.batch_size, self.hidden_size)\n",
    "        c_photo = torch.zeros(self.batch_size, self.hidden_size)\n",
    "        h_bipol = torch.zeros(self.batch_size, 1)\n",
    "        c_bipol = torch.zeros(self.batch_size, 1)\n",
    "\n",
    "        h_photo, c_photo, h_bipol, c_bipol  =  (h_photo.to(self.device), c_photo.to(self.device),\n",
    "                                                h_bipol.to(self.device), c_bipol.to(self.device))\n",
    "        return h_photo, c_photo, h_bipol, c_bipol\n",
    "\n",
    "    def forward(self, stimulus):\n",
    "        h_photo, c_photo, h_bipol, c_bipol = self.init_weights()\n",
    "        output = torch.empty(stimulus.size())\n",
    "        for i in range(stimulus.shape[0]):\n",
    "            h_photo, c_photo = self.photo(stimulus[i], (h_photo, c_photo))\n",
    "            h_bipol, c_bipol = self.bipol(h_photo, (h_bipol, c_bipol))\n",
    "            output[i] = h_bipol\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of loading one type of cell, for CPU uncomment/comment the corresponding lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "#device = torch.device(\"cpu\")\n",
    "root = \"./models/lstm/\"\n",
    "file = \"bipolar_type_1_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_173563030.pt\"\n",
    "model_type1 = Lstmcell(device, batch_size=8)\n",
    "#loadmodel(model_type1, file, root=root)\n",
    "loadmodel(model_type1, file, root=root, cuda=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading all cell types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"bipolar_type_10_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_141844336.pt\",\n",
    "\"bipolar_type_10_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_15283127.pt\",\n",
    "\"bipolar_type_10_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_269304424.pt\",\n",
    "\"bipolar_type_11_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_330601724.pt\",\n",
    "\"bipolar_type_11_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_6552616.pt\",\n",
    "\"bipolar_type_11_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_906165115.pt\",\n",
    "\"bipolar_type_12_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_241280767.pt\",\n",
    "\"bipolar_type_12_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_531998175.pt\",\n",
    "\"bipolar_type_12_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_66948984.pt\",\n",
    "\"bipolar_type_13_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_179780013.pt\",\n",
    "\"bipolar_type_13_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_29671914.pt\",\n",
    "\"bipolar_type_13_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_957957141.pt\",\n",
    "\"bipolar_type_14_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_1098300983.pt\",\n",
    "\"bipolar_type_14_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_715986001.pt\",\n",
    "\"bipolar_type_14_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_950670156.pt\",\n",
    "\"bipolar_type_1_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_173563030.pt\",\n",
    "\"bipolar_type_1_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_533793902.pt\",\n",
    "\"bipolar_type_1_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_560106992.pt\",\n",
    "\"bipolar_type_2_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_1189464144.pt\",\n",
    "\"bipolar_type_2_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_1386305591.pt\",\n",
    "\"bipolar_type_2_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_1690116631.pt\",\n",
    "\"bipolar_type_3_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_6251400.pt\",\n",
    "\"bipolar_type_3_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_766344198.pt\",\n",
    "\"bipolar_type_4_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_1128190960.pt\",\n",
    "\"bipolar_type_4_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_1303619795.pt\",\n",
    "\"bipolar_type_5_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_15785680.pt\",\n",
    "\"bipolar_type_5_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_162832312.pt\",\n",
    "\"bipolar_type_5_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_275131426.pt\",\n",
    "\"bipolar_type_6_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_1166721910.pt\",\n",
    "\"bipolar_type_6_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_245239075.pt\",\n",
    "\"bipolar_type_6_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_75266929.pt\",\n",
    "\"bipolar_type_7_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_126872303.pt\",\n",
    "\"bipolar_type_7_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_3808957.pt\",\n",
    "\"bipolar_type_7_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_549864573.pt\",\n",
    "\"bipolar_type_8_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_159725155.pt\",\n",
    "\"bipolar_type_8_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_907167403.pt\",\n",
    "\"bipolar_type_8_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_998040308.pt\",\n",
    "\"bipolar_type_9_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_133931557.pt\",\n",
    "\"bipolar_type_9_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_74535517.pt\",\n",
    "\"bipolar_type_9_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_783677.pt\",\n",
    "\"ganglionar_type_10_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_237669104.pt\",\n",
    "\"ganglionar_type_10_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_411326535.pt\",\n",
    "\"ganglionar_type_10_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_905665101.pt\",\n",
    "\"ganglionar_type_11_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_1239763412.pt\",\n",
    "\"ganglionar_type_11_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_563475798.pt\",\n",
    "\"ganglionar_type_11_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_82775029.pt\",\n",
    "\"ganglionar_type_12_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_172090477.pt\",\n",
    "\"ganglionar_type_12_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_381057631.pt\",\n",
    "\"ganglionar_type_12_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_839874706.pt\",\n",
    "\"ganglionar_type_13_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_1241648508.pt\",\n",
    "\"ganglionar_type_13_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_1494485751.pt\",\n",
    "\"ganglionar_type_13_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_165704750.pt\",\n",
    "\"ganglionar_type_14_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_183174642.pt\",\n",
    "\"ganglionar_type_14_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_491223730.pt\",\n",
    "\"ganglionar_type_14_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_94689407.pt\",\n",
    "\"ganglionar_type_15_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_1177464148.pt\",\n",
    "\"ganglionar_type_15_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_138207895.pt\",\n",
    "\"ganglionar_type_15_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_249169987.pt\",\n",
    "\"ganglionar_type_16_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_1122989673.pt\",\n",
    "\"ganglionar_type_16_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_1612269384.pt\",\n",
    "\"ganglionar_type_16_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_584220802.pt\",\n",
    "\"ganglionar_type_17_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_160596147.pt\",\n",
    "\"ganglionar_type_17_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_705127268.pt\",\n",
    "\"ganglionar_type_17_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_989493600.pt\",\n",
    "\"ganglionar_type_18_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_321926665.pt\",\n",
    "\"ganglionar_type_18_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_6292319.pt\",\n",
    "\"ganglionar_type_18_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_78265144.pt\",\n",
    "\"ganglionar_type_19_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_1175274272.pt\",\n",
    "\"ganglionar_type_19_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_156834873.pt\",\n",
    "\"ganglionar_type_19_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_329472619.pt\",\n",
    "\"ganglionar_type_1_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_198326626.pt\",\n",
    "\"ganglionar_type_1_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_364471185.pt\",\n",
    "\"ganglionar_type_1_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_600241632.pt\",\n",
    "\"ganglionar_type_20_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_413860435.pt\",\n",
    "\"ganglionar_type_20_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_44144047.pt\",\n",
    "\"ganglionar_type_20_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_9231279.pt\",\n",
    "\"ganglionar_type_21_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_1369609959.pt\",\n",
    "\"ganglionar_type_21_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_239711620.pt\",\n",
    "\"ganglionar_type_21_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_619669146.pt\",\n",
    "\"ganglionar_type_22_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_380063816.pt\",\n",
    "\"ganglionar_type_22_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_413063837.pt\",\n",
    "\"ganglionar_type_22_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_9730860.pt\",\n",
    "\"ganglionar_type_23_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_1211777820.pt\",\n",
    "\"ganglionar_type_23_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_1482063350.pt\",\n",
    "\"ganglionar_type_23_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_468143651.pt\",\n",
    "\"ganglionar_type_24_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_117458979.pt\",\n",
    "\"ganglionar_type_24_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_2085822589.pt\",\n",
    "\"ganglionar_type_24_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_28653427.pt\",\n",
    "\"ganglionar_type_25_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_1089518756.pt\",\n",
    "\"ganglionar_type_25_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_1599144784.pt\",\n",
    "\"ganglionar_type_25_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_164781825.pt\",\n",
    "\"ganglionar_type_26_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_131237893.pt\",\n",
    "\"ganglionar_type_26_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_172340461.pt\",\n",
    "\"ganglionar_type_26_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_350523332.pt\",\n",
    "\"ganglionar_type_27_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_479449101.pt\",\n",
    "\"ganglionar_type_27_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_55482899.pt\",\n",
    "\"ganglionar_type_27_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_84281034.pt\",\n",
    "\"ganglionar_type_28_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_264065688.pt\",\n",
    "\"ganglionar_type_28_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_463811580.pt\",\n",
    "\"ganglionar_type_28_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_566859297.pt\",\n",
    "\"ganglionar_type_29_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_152841965.pt\",\n",
    "\"ganglionar_type_29_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_530384460.pt\",\n",
    "\"ganglionar_type_29_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_87052115.pt\",\n",
    "\"ganglionar_type_2_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_295277855.pt\",\n",
    "\"ganglionar_type_2_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_36236986.pt\",\n",
    "\"ganglionar_type_2_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_43875023.pt\",\n",
    "\"ganglionar_type_30_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_109044538.pt\",\n",
    "\"ganglionar_type_30_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_289226367.pt\",\n",
    "\"ganglionar_type_30_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_556781329.pt\",\n",
    "\"ganglionar_type_31_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_178961346.pt\",\n",
    "\"ganglionar_type_31_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_331931885.pt\",\n",
    "\"ganglionar_type_31_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_423348922.pt\",\n",
    "\"ganglionar_type_32_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_1213524776.pt\",\n",
    "\"ganglionar_type_32_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_267392019.pt\",\n",
    "\"ganglionar_type_32_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_438881270.pt\",\n",
    "\"ganglionar_type_33_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_1135858999.pt\",\n",
    "\"ganglionar_type_33_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_268015804.pt\",\n",
    "\"ganglionar_type_33_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_465011534.pt\",\n",
    "\"ganglionar_type_34_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_1369503192.pt\",\n",
    "\"ganglionar_type_34_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_1539395724.pt\",\n",
    "\"ganglionar_type_34_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_1974221233.pt\",\n",
    "\"ganglionar_type_35_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_131716857.pt\",\n",
    "\"ganglionar_type_35_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_944573925.pt\",\n",
    "\"ganglionar_type_35_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_991169481.pt\",\n",
    "\"ganglionar_type_36_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_1442639811.pt\",\n",
    "\"ganglionar_type_36_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_1731006758.pt\",\n",
    "\"ganglionar_type_36_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_992378617.pt\",\n",
    "\"ganglionar_type_37_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_484059010.pt\",\n",
    "\"ganglionar_type_37_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_608691745.pt\",\n",
    "\"ganglionar_type_37_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_744266624.pt\",\n",
    "\"ganglionar_type_38_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_247298880.pt\",\n",
    "\"ganglionar_type_38_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_39931331.pt\",\n",
    "\"ganglionar_type_38_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_535897221.pt\",\n",
    "\"ganglionar_type_39_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_1069691165.pt\",\n",
    "\"ganglionar_type_39_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_1760278679.pt\",\n",
    "\"ganglionar_type_39_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_54671615.pt\",\n",
    "\"ganglionar_type_3_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_2062632841.pt\",\n",
    "\"ganglionar_type_3_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_532270609.pt\",\n",
    "\"ganglionar_type_3_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_797123245.pt\",\n",
    "\"ganglionar_type_4_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_100781008.pt\",\n",
    "\"ganglionar_type_4_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_244493523.pt\",\n",
    "\"ganglionar_type_4_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_402066829.pt\",\n",
    "\"ganglionar_type_5_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_1253775427.pt\",\n",
    "\"ganglionar_type_5_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_198058435.pt\",\n",
    "\"ganglionar_type_5_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_702858507.pt\",\n",
    "\"ganglionar_type_6_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_1571939542.pt\",\n",
    "\"ganglionar_type_6_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_1580361806.pt\",\n",
    "\"ganglionar_type_6_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_549525671.pt\",\n",
    "\"ganglionar_type_7_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_1186972223.pt\",\n",
    "\"ganglionar_type_7_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_1488177546.pt\",\n",
    "\"ganglionar_type_7_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_177417729.pt\",\n",
    "\"ganglionar_type_8_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_130857530.pt\",\n",
    "\"ganglionar_type_8_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_222439868.pt\",\n",
    "\"ganglionar_type_8_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_565791624.pt\",\n",
    "\"ganglionar_type_9_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_102671336.pt\",\n",
    "\"ganglionar_type_9_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_666754141.pt\",\n",
    "\"ganglionar_type_9_net_lstm_hiddensize_51_epochs_30_batches_8_lr_0.001_68539956.pt\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "root = \"./models/lstm/\"\n",
    "device = torch.device(\"cuda:0\")\n",
    "#device = torch.device(\"cpu\")\n",
    "for file in files:\n",
    "    name = file[:file.find(\"net\") - 1]\n",
    "    models[name] = Lstmcell(device, batch_size=8)\n",
    "    loadmodel(models[name], file, root=root, cuda=True)\n",
    "    #loadmodel(models[name], file, root=root, cuda=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example for a user-created stimulus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = np.zeros((182, 288, 384))\n",
    "for i, n in enumerate(range(200,382)):\n",
    "    frame = imageio.imread('dataset/images/mall/EnterExitCrossingPaths1cor0{}.jpg'.format(n))\n",
    "    frames[i] = frame[:,:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample image from the series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(frames[0], cmap = 'Greys_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will crop the images and use only the green channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(frames[0,10:-1:1,10:300:1], cmap = 'Greys_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And downsample to use fewer neurons. We will use three levels of downsampling to emulate neurons with different size of receptive fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(frames[0,44:244:4,40:252:4], cmap = 'Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_frames = frames[:120,28:,40:300:1]\n",
    "m_frames = frames[:120,28:-1:2,40:300:2]\n",
    "l_frames = frames[:120,28:-1:4,40:300:4]\n",
    "s_frames.shape, m_frames.shape, l_frames.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following is very slow because we only have one network instance, so we iterate over pixels sequentally. We need to improve this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "pre = 6\n",
    "resp_mov_l = np.zeros((l_frames.shape[0]+pre,l_frames.shape[1],l_frames.shape[2]))\n",
    "for x in range(l_frames.shape[1]):\n",
    "    for y in range(l_frames.shape[2]):\n",
    "        #print('processing pixel x={} y={}'.format(x,y))\n",
    "        stim = np.concatenate((np.ones(pre)*128, l_frames[:,x,y]))\n",
    "        stimulus_t = transform_data(stim, 8)\n",
    "        out = models[\"ganglionar_type_{}\".format(n)](stimulus_t.to(device))\n",
    "        resp_mov_l[:,x,y] = out[:,0,:].cpu().detach().numpy()[:,0]\n",
    "        resp_mov_l[:,x,y] = resp_mov_l[:,x,y] - resp_mov_l[:,x,y].mean()\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "pre = 6\n",
    "resp_mov = np.zeros((s_frames.shape[0]+pre,s_frames.shape[1],s_frames.shape[2]))\n",
    "for x in range(s_frames.shape[1]):\n",
    "    for y in range(s_frames.shape[2]):\n",
    "        #print('processing pixel x={} y={}'.format(x,y))\n",
    "        stim = np.concatenate((np.ones(pre)*128, s_frames[:,x,y]))\n",
    "        stimulus_t = transform_data(stim, 8)\n",
    "        out = models[\"ganglionar_type_{}\".format(n)](stimulus_t.to(device))\n",
    "        resp_mov[:,x,y] = out[:,0,:].cpu().detach().numpy()[:,0]\n",
    "        resp_mov[:,x,y] = resp_mov[:,x,y] - resp_mov[:,x,y].mean()\n",
    "        #print('done!')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import convis\n",
    "convis.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spk = convis.filters.spiking.Poisson()\n",
    "o = spk.run(resp_mov / resp_mov.max())\n",
    "plt.figure()\n",
    "o.plot(mode='lines')\n",
    "spk_out = o.array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "pre = 6\n",
    "resp_mov_m = np.zeros((m_frames.shape[0]+pre,m_frames.shape[1],m_frames.shape[2]))\n",
    "for x in range(m_frames.shape[1]):\n",
    "    for y in range(m_frames.shape[2]):\n",
    "        #print('processing pixel x={} y={}'.format(x,y))\n",
    "        stim = np.concatenate((np.ones(pre)*128, m_frames[:,x,y]))\n",
    "        stimulus_t = transform_data(stim, 8)\n",
    "        out = models[\"ganglionar_type_{}\".format(n)](stimulus_t.to(device))\n",
    "        resp_mov_m[:,x,y] = out[:,0,:].cpu().detach().numpy()[:,0]\n",
    "resp_mov_m = resp_mov_m - resp_mov_m[:6].mean()\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spk = convis.filters.spiking.Izhikevich()\n",
    "o_m = spk.run(resp_mov_m[:,:,:]/resp_mov_m[:,:,:].max())\n",
    "plt.figure()\n",
    "o_m.plot(mode='lines')\n",
    "spk_out_m = o_m.array()\n",
    "spk_out_m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spk = convis.filters.spiking.Izhikevich()\n",
    "o_m_off = spk.run(-resp_mov_m[:,:,:]/resp_mov_m[:,:,:].max())\n",
    "plt.figure()\n",
    "o_m_off.plot(mode='lines')\n",
    "spk_out_m_off = o_m_off.array()\n",
    "spk_out_m_off.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spk = convis.filters.spiking.Izhikevich()\n",
    "o_l = spk.run(resp_mov_l[:,:,:]/resp_mov_l[:,:,:].max())\n",
    "plt.figure()\n",
    "o_l.plot(mode='lines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spk_out_l = o_l.array()\n",
    "spk_out_l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(s_frames.shape[2]):\n",
    "    fig, ax = plt.subplots(1,3)\n",
    "    fig.set_size_inches(9,3)\n",
    "    ax[0].imshow(s_frames[i], cmap='gray')\n",
    "    ax[0].axis('off')\n",
    "    ax[1].imshow(resp_mov[i+6], vmin=resp_mov[6:,:,:].min(), vmax=resp_mov[6:,:,:].max())\n",
    "    ax[1].axis('off')\n",
    "    ax[2].imshow(spk_out[0,0,i+6])\n",
    "    #ax[1].imshow(s_frames[i+6])\n",
    "    ax[2].axis('off')\n",
    "    fig.subplots_adjust(hspace=0, wspace=0)\n",
    "    fig.savefig('outputs/output_gangliontype3_s_{:03d}.png'.format(i), facecolor='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(l_frames.shape[0]):\n",
    "    fig, ax = plt.subplots(1,3)\n",
    "    fig.set_size_inches(9,3)\n",
    "    ax[0].imshow(l_frames[i], cmap='gray')\n",
    "    ax[0].axis('off')\n",
    "    ax[1].imshow(resp_mov_l[i+6,:,:], vmin=resp_mov_l[6:,:,:].min(), vmax=resp_mov_l[6:,:,:].max())\n",
    "    ax[1].axis('off')\n",
    "    ax[2].imshow(spk_out[0,0,i+6])\n",
    "    ax[2].axis('off')\n",
    "    fig.subplots_adjust(hspace=0, wspace=0)\n",
    "    fig.savefig('outputs/output_gangliontype3_l_{:03d}.png'.format(i), facecolor='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(m_frames.shape[0]):\n",
    "    fig, ax = plt.subplots(1,3)\n",
    "    fig.set_size_inches(9,3)\n",
    "    ax[0].imshow(m_frames[i], cmap='gray')\n",
    "    ax[0].axis('off')\n",
    "    ax[1].imshow(resp_mov_m[i+6,:,:], vmin=resp_mov_m[6:,:,:].min(), vmax=resp_mov_m[6:,:,:].max())\n",
    "    ax[1].axis('off')\n",
    "    ax[2].imshow(spk_out_m[0,0,i+6])\n",
    "    ax[2].axis('off')\n",
    "    fig.subplots_adjust(hspace=0, wspace=0)\n",
    "    fig.savefig('outputs/output_gangliontype5_m_{:03d}.png'.format(i), facecolor='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(4,4)\n",
    "ims = []\n",
    "for i in range(spk_out.shape[2]):\n",
    "    im = ax.imshow(spk_out[0,0,i], animated=True)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ims.append([im])\n",
    "fig.tight_layout()   \n",
    "ani = animation.ArtistAnimation(fig, ims, interval=50, blit=True,\n",
    "                                repeat_delay=1000)\n",
    "ani.save('output_t3nat.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 15\n",
    "resp15_mov = np.zeros((200,s_frames.shape[1],s_frames.shape[2]))\n",
    "for x in range(s_frames.shape[1]):\n",
    "    #print('processing column x={}'.format(x))\n",
    "    for y in range(s_frames.shape[2]):\n",
    "        #print('processing pixel x={} y={}'.format(x,y))\n",
    "        stim = np.concatenate((np.ones(18)*128, s_frames[:,x,y]))\n",
    "        stimulus_t = transform_data(stim, 8)\n",
    "        out = models[\"ganglionar_type_{}\".format(n)](stimulus_t.to(device))\n",
    "        resp15_mov[:,x,y] = out[:,0,:].cpu().detach().numpy()[:,0]\n",
    "print('done!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_size_inches(4,4)\n",
    "ims15 = []\n",
    "for i in range(resp15_mov.shape[0]):\n",
    "    print('processing frame {}'.format(i))\n",
    "    im1 = plt.imshow(resp15_mov[i], animated=True, cmap = 'inferno', vmin=resp15_mov.min(), vmax=resp15_mov.max())\n",
    "    ims15.append([im1])\n",
    "    print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani15 = animation.ArtistAnimation(fig, ims15, interval=50, blit=True,\n",
    "                                repeat_delay=1000)\n",
    "ani15.save('output_t15Bxs.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 34\n",
    "resp34_mov = np.zeros((200,s_frames.shape[1],s_frames.shape[2]))\n",
    "for x in range(s_frames.shape[1]):\n",
    "    for y in range(s_frames.shape[2]):\n",
    "        print('processing pixel x={} y={}'.format(x,y))\n",
    "        stim = np.concatenate((np.ones(18)*128, s_frames[:,x,y]))\n",
    "        stimulus_t = transform_data(stim, 8)\n",
    "        out = models[\"ganglionar_type_{}\".format(n)](stimulus_t.to(device))\n",
    "        resp34_mov[:,x,y] = out[:,0,:].cpu().detach().numpy()[:,0]\n",
    "        print('done!')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ims34 = []\n",
    "for i in range(resp34_mov.shape[0]):\n",
    "    print('processing frame {}'.format(i))\n",
    "    im1 = plt.imshow(resp34_mov[i], animated=True, cmap = 'cividis', vmin=resp34_mov.min(), vmax=resp34_mov.max())\n",
    "    ims34.append([im1])\n",
    "    print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani34 = animation.ArtistAnimation(fig, ims34, interval=50, blit=True,\n",
    "                                repeat_delay=1000)\n",
    "ani34.save('output_t34B.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "resp_b4_mov = np.zeros((200,m_frames.shape[1],m_frames.shape[2]))\n",
    "for x in range(m_frames.shape[1]):\n",
    "    for y in range(m_frames.shape[2]):\n",
    "        #print('processing pixel x={} y={}'.format(x,y))\n",
    "        stim = np.concatenate((np.ones(18)*128, m_frames[:,x,y]))\n",
    "        stimulus_t = transform_data(stim, 8)\n",
    "        out = models[\"bipolar_type_{}\".format(n)](stimulus_t.to(device))\n",
    "        resp_b4_mov[:,x,y] = out[:,0,:].cpu().detach().numpy()[:,0]\n",
    "print('done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_size_inches(4,4)\n",
    "ims_b4 = []\n",
    "for i in range(resp_b4_mov.shape[0]):\n",
    "    #print('processing frame {}'.format(i))\n",
    "    im1 = plt.imshow(resp_b4_mov[i], animated=True, cmap = 'viridis', vmin=0, vmax=1)\n",
    "    ims_b4.append([im1])\n",
    "    \n",
    "ani_b4 = animation.ArtistAnimation(fig, ims_b4, interval=50, blit=True,\n",
    "                                repeat_delay=1000)\n",
    "ani_b4.save('output_b4.mp4')\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in small_cells:\n",
    "    print('processing cell {}'.format(n))\n",
    "    resp_mov = np.zeros((200,s_frames.shape[1],s_frames.shape[2]))\n",
    "    for x in range(s_frames.shape[1]):\n",
    "        for y in range(s_frames.shape[2]):\n",
    "            stim = np.concatenate((np.ones(18)*128, s_frames[:,x,y]))\n",
    "            stimulus_t = transform_data(stim, 8)\n",
    "            out = models[\"ganglionar_type_{}\".format(n)](stimulus_t.to(device))\n",
    "            resp_mov[:,x,y] = out[:,0,:].cpu().detach().numpy()[:,0]\n",
    "            \n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(3,3)\n",
    "    imsT = []\n",
    "    for i in range(resp_mov.shape[0]):\n",
    "        imT = plt.imshow(resp_mov[i], animated=True, cmap = 'inferno', vmin=resp_mov.min(), vmax=resp_mov.max())\n",
    "        imsT.append([imT])\n",
    "        \n",
    "    ani_T = animation.ArtistAnimation(fig, imsT, interval=50, blit=True,\n",
    "                                repeat_delay=1000)\n",
    "    ani_T.save('videos/output_g{}i.mp4'.format(n))        \n",
    "    plt.close(fig)\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in medium_cells:\n",
    "    print('processing cell {}'.format(n))\n",
    "    resp_mov = np.zeros((200,m_frames.shape[1],m_frames.shape[2]))\n",
    "    for x in range(m_frames.shape[1]):\n",
    "        for y in range(m_frames.shape[2]):\n",
    "            stim = np.concatenate((np.ones(18)*128, m_frames[:,x,y]))\n",
    "            stimulus_t = transform_data(stim, 8)\n",
    "            out = models[\"ganglionar_type_{}\".format(n)](stimulus_t.to(device))\n",
    "            resp_mov[:,x,y] = out[:,0,:].cpu().detach().numpy()[:,0]\n",
    "            \n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(3,3)\n",
    "    imsT = []\n",
    "    for i in range(resp_mov.shape[0]):\n",
    "        imT = plt.imshow(resp_mov[i], animated=True, cmap = 'inferno', vmin=resp_mov.min(), vmax=resp_mov.max())\n",
    "        imsT.append([imT])\n",
    "        \n",
    "    ani_T = animation.ArtistAnimation(fig, imsT, interval=50, blit=True,\n",
    "                                repeat_delay=1000)\n",
    "    ani_T.save('videos/output_g{}i.mp4'.format(n))        \n",
    "    plt.close(fig)\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in large_cells:\n",
    "    print('processing cell {}'.format(n))\n",
    "    resp_mov = np.zeros((200,l_frames.shape[1],l_frames.shape[2]))\n",
    "    for x in range(l_frames.shape[1]):\n",
    "        for y in range(l_frames.shape[2]):\n",
    "            stim = np.concatenate((np.ones(18)*128, l_frames[:,x,y]))\n",
    "            stimulus_t = transform_data(stim, 8)\n",
    "            out = models[\"ganglionar_type_{}\".format(n)](stimulus_t.to(device))\n",
    "            resp_mov[:,x,y] = out[:,0,:].cpu().detach().numpy()[:,0]\n",
    "            \n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(3,3)\n",
    "    imsT = []\n",
    "    for i in range(resp_mov.shape[0]):\n",
    "        imT = plt.imshow(resp_mov[i], animated=True, cmap = 'inferno', vmin=resp_mov.min(), vmax=resp_mov.max())\n",
    "        imsT.append([imT])\n",
    "        \n",
    "    ani_T = animation.ArtistAnimation(fig, imsT, interval=50, blit=True,\n",
    "                                repeat_delay=1000)\n",
    "    ani_T.save('videos/output_g{}i.mp4'.format(n))        \n",
    "    plt.close(fig)\n",
    "print('done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
